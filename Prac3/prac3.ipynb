{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP2200/COMP6200 Prac - Week 3\n",
    "\n",
    "_Author: Benjamin Pope (benjamin.pope@mq.edu.au)_\n",
    "\n",
    "---\n",
    "\n",
    "In lectures this week we encountered ideas about data visualization, including histograms and scatter plots; and about distributions, including the normal distribution and the central limit theorem. \n",
    "\n",
    "Today we are going to explore the Monte Carlo method: using random numbers to estimate the properties of a distribution, and use this to illustrate some important distributions and the central limit theorem, and how to propagate uncertainties through calculations.\n",
    "\n",
    "**Please submit the completed notebook for this workshop to your GitHub Classroom for marking!**\n",
    "\n",
    "Remember that to use a notebook, you need to run each cell. If left side of a cell says `[ ]`, it hasn't been run. If the left side\n",
    "says something like `[25]` then it has been. Shift-Enter runs a cell;  if you look around the user interface, various other menu items can run several cells in sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # maths\n",
    "import matplotlib.pyplot as plt # plotting\n",
    "\n",
    "import pandas as pd # data manipulation\n",
    "import seaborn as sns # plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `numpy` and `scipy` libraries have many functions for generating random numbers (as do many packages in many languages). Let's become familiar with some of the common functions, and take a tour through some of the important distributions:\n",
    "\n",
    "- `np.random.rand()` generates a random number between 0 and 1\n",
    "    - this is used to model continuous data that is uniformly distributed\n",
    "- `np.random.binomial(n, p)` generates a random number from a binomial distribution with parameters `n` and `p`\n",
    "    - this is used to model yes/no events like coin flips, where `n` is the number of flips and `p` is the probability of heads\n",
    "- `np.random.beta(a, b)` generates a random number from a beta distribution with parameters `a` and `b`\n",
    "    - this is used to model probabilities, where `a` and `b` are the number of successes and failures\n",
    "- `np.random.randn()` generates a random number from a standard normal distribution (mean 0, standard deviation 1)\n",
    "    - this is used to model continuous data that is normally distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pseudorandom numbers in computing are generated by deterministic algorithms, where the next step is the application of an equation to the previous step.\n",
    "\n",
    "We need to set \n",
    "\n",
    "- a seed for the random number generator, so that we can reproduce the same results each time we run the code\n",
    "- the number of random numbers we want to generate\n",
    "\n",
    "In this part of the exercise, you want to \n",
    "\n",
    "- try out what these look like with different seeds, and \n",
    "- see how the histograms converge to a uniform distribution as the number of random numbers increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first let's simulate some uniform data\n",
    "\n",
    "np.random.seed(2) # set the seed - play with changing this \n",
    "\n",
    "n = 100 # number of samples\n",
    "\n",
    "# generate some random data\n",
    "x = np.random.rand(n)\n",
    "print(x) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, plot a time series of these random samples - are they correlated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a histogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also a family of functions in `scipy.stats` for statistics, which can draw samples but also calculate probability densit functions (PDFs), cumulative distribution functions (CDFs), and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate samples, we use the `.rvs()` method, which stands for \"random variates\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draws = stats.uniform.rvs(size=10, random_state=2) # see that with the same seed we get the same data as numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, edit this to show that as you increase the size of the draw, you approach the expected distribution more and more closely.\n",
    "\n",
    "How big does the draw need to be for all bins to be within 1% of the expected value?\n",
    "\n",
    "Answer: ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = stats.uniform() # scipy is object oriented, so we need to create an object to work with\n",
    "draws = dist.rvs(size=1000, random_state=3) # see that with the same seed we get the same data as numpy\n",
    "x = np.linspace(0, 1, 1000)\n",
    "pdf = dist.pdf(x) # probability density function\n",
    "\n",
    "plt.hist(draws,density=True) # density=True normalizes the histogram\n",
    "plt.plot(x, pdf, 'r-')\n",
    "plt.xlim(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also calculate the *cumulative distribution function* (CDF) of the data, which is the probability that a random variable is less than or equal to a certain value. This is most vitally important for calculating p-values in hypothesis testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf = dist.cdf(x) # cumulative density function\n",
    "plt.plot(x, cdf, 'r-')\n",
    "plt.xlim(x.min(), x.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a few other distributions:\n",
    "- exponential \n",
    "- Poisson\n",
    "- binomial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exponential distribution represents the waiting time between discrete events that occur at random but at a constant average rate. \n",
    "\n",
    "This can represent\n",
    "\n",
    "- bus arrival times\n",
    "- radioactive decay\n",
    "- shot noise in electronics or photon counting optics\n",
    "- raindrops hitting a window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = stats.expon(scale=1) # exponential distribution with scale=1\n",
    "draws = dist.rvs(size=10000, random_state=3) # see that with the same seed we get the same data as numpy\n",
    "x = np.linspace(0, 10, 1000)\n",
    "pdf = dist.pdf(x) # probability density function\n",
    "\n",
    "plt.hist(draws,density=True,bins=20) # density=True normalizes the histogram\n",
    "plt.plot(x, pdf, 'r-')\n",
    "plt.xlim(0,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the CDF look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, dist.cdf(x), 'r-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an amazing trick with CDFs: if you take the inverse of the CDF, you can generate random numbers from that distribution by looking up the value of the CDF at a random number between 0 and 1. You can do this with the `.ppf()` method, which stands for \"percent point function\" - this calculates what value the percentiles of the distribution correspond to.\n",
    "\n",
    "This is how a lot of the functions in `scipy` generate random variates under the hood. This is called the *inverse transform method*. Let's try this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniform_samples = np.random.rand(1000)\n",
    "plt.hist(dist.ppf(uniform_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also convenience functions for calculating summary statistics like the mean and standard deviation exactly, without having to draw samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"mean\",dist.mean(), \"sigma\",dist.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are vectorized, so if you pass it a range of parameters, it will return a range of results. Let's plot this to see how the mean varies with $a$ and $b$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is closely related to a discrete distribution: the Poisson distribution, which is the probability of getting a number of events arriving at random at a fixed rate called $\\lambda$ or $\\mu$ (ie like in the exponential distribution) *over a fixed interval of time*.\n",
    "\n",
    "Play with the mean value: for small samples this is very lumpy and discrete, but for large count rates it is converging to something very familiar! \n",
    "\n",
    "Why do you think this is? \n",
    "\n",
    "*Answer*: ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate = 3\n",
    "dist = stats.poisson(mu=rate) # poisson distribution with mu=3\n",
    "draws = dist.rvs(size=10000, random_state=3) # see that with the same seed we get the same data as numpy\n",
    "x = np.arange(0, np.round(rate*3))\n",
    "pmf = dist.pmf(x) # probability mass function for discrete distributions\n",
    "\n",
    "plt.hist(draws,density=True,bins=20) # density=True normalizes the histogram\n",
    "plt.plot(x, pmf, 'r-')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, plot how the mean and standard deviation of a Poisson distribution vary with the rate parameter $\\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BONUS QUESTION: can you show numerically that the Poisson distribution is the distribution of the number of events in a fixed interval of time if the events are exponentially distributed in time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many more distributions implemented in `scipy.stats`, but one of the most important is the binomial distribution, which is the probability of getting $k$ successes in $n$ trials, each with probability $p$ of success. This models the number of heads in $n$ coin flips ($p=0.5$), or ($p = 1/6$) the number of times a die comes up 6 in $n$ rolls.\n",
    "\n",
    "Check how this varies with $n$ and $p$. You'll notice that for small $n$ this is very blocky - but for large $n$ it is converging to something very familiar! \n",
    "\n",
    "Why do you think this is? \n",
    "\n",
    "*Answer*: ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, p = 100, 0.2\n",
    "dist = stats.binom(n=n, p=p) # binomial distribution with n trials and p probability\n",
    "draws = dist.rvs(size=10000, random_state=3) \n",
    "x = np.arange(0, np.max([np.round(n*p*2),np.max(draws)]))\n",
    "pmf = dist.pmf(x) # probability mass function = probability density function for discrete distributions\n",
    "\n",
    "plt.hist(draws,density=True,bins=20) # density=True normalizes the histogram\n",
    "plt.plot(x, pmf, 'r-')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, plot how the mean and standard deviation of a binomial distribution vary with the number of trials $n$ for some different values of $p$ of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns = [10, 100, 1000]\n",
    "ps = [0.2, 0.5, 0.8]\n",
    "\n",
    "for p in ps:\n",
    "    dist = stats.binom(n=ns, p=p) # binomial distribution with n trials and p probability\n",
    "    plt.plot(ns, dist.mean(), 'o-', label=f'p={p}')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Central Limit Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In class we talked about the central limit theorem: the sum of a large number of independent random variables is approximately normally distributed, and that you have mean and standard deviation \n",
    "\n",
    "$$\n",
    "\\mu = N \\times \\mu_0, \\quad \\sigma = \\sqrt{N} \\times \\sigma_0\n",
    "$$\n",
    "\n",
    "The previous sections showed that this comes naturally out of large samples from Poisson or binomial distributions. \n",
    "\n",
    "Here's how we can draw from normal distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sig = 0, 1\n",
    "dist = stats.norm(mu, sig) # normal distribution with mean=0 and sigma=1\n",
    "draws = dist.rvs(size=10000, random_state=3) \n",
    "x = np.linspace(mu-4*sig, mu+4*sig, 1000)\n",
    "pdf = dist.pdf(x) # probability mass function = probability density function for discrete distributions\n",
    "\n",
    "plt.hist(draws,density=True,bins=20) # density=True normalizes the histogram\n",
    "plt.plot(x, pdf, 'r-')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, use the analytic distribution means and variances to demonstrate the central limit theorem holds for:\n",
    "\n",
    "- sum of N draws from uniform distributions\n",
    "- sum of N draws frmo exponential distributions\n",
    "- sum of N draws from uniform  and M draws from exponential distributions\n",
    "\n",
    "and quantify how big N and M need to be for the central limit theorem to hold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One *vitally important* use for the normal distribution is in hypothesis testing - so the cumulative distribution function of the normal has a special name, the error function, and is implemented in `scipy.special`.\n",
    "\n",
    "The error function is the integral of the normal distribution, and is used to calculate the probability that a random variable is within a certain range. In particular, everybody in stats will use $1\\sigma$, $2\\sigma$, and $3\\sigma$ intervals as quantiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sig = 0, 1\n",
    "dist = stats.norm(mu, sig) # normal distribution with mean=0 and sigma=1\n",
    "\n",
    "x = np.linspace(mu-4*sig, mu+4*sig, 1000)\n",
    "pdf = dist.pdf(x) # probability mass function = probability density function for discrete distributions\n",
    "cdf = dist.cdf(x) # cumulative density function\n",
    "plt.plot(x, pdf, 'r-')\n",
    "plt.plot(x, cdf, 'b-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, calculate the 1, 2, 3, 4, 5 $\\sigma$ quantiles for a normal distribution. How frequent are \"5-sigma\" events?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Distributions\n",
    "\n",
    "We can also consider multivariate distributions, where we have more than one random variable. Let's consider the multivariate normal, in 2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = [0, 0]\n",
    "cov = [[1, 0], [0, 1]]\n",
    "dist = stats.multivariate_normal(mean=mu, cov=cov)\n",
    "draws = dist.rvs(size=100, random_state=3)\n",
    "\n",
    "plt.scatter(draws[:,0],draws[:,1])\n",
    "plt.axis('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make this into a `pd` dataframe and plot it as a corner plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(draws, columns=['x', 'y'])\n",
    "sns.pairplot(df, kind='kde') # play with this - try kind='scatter', 'kde', or 'hist'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above we have a few different options for the pairplot. In the markdown below, say why you might prefer one over another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `kind='scatter'`:\n",
    "    - good for small data sets\n",
    "- `kind='hist'`:\n",
    "    - good for medium sized data sets \n",
    "- `kind='kde'`:\n",
    "    - good for smoothing large data sets, but slow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That expression `cov` is called the *covariance matrix*, and it describes how the two variables are correlated. The diagonal terms of that matrix are the *marginal* variances of the two variables, and the off-diagonal terms are the *covariance* between the two variables. The matrix is always symmetric, and can be written in terms of a correlation coefficient $\\rho$:\n",
    "\n",
    "$$\n",
    "\\text{cov} = \\begin{pmatrix} \\sigma_1^2 & \\rho \\sigma_1 \\sigma_2 \\\\ \\rho \\sigma_1 \\sigma_2 & \\sigma_2^2 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "where $\\rho$ is the correlation coefficient, which is always between -1 and 1.\n",
    "\n",
    "In the next cell, explore \n",
    "\n",
    "- what does a multivariate normal look like for $\\rho \\in (-1, 0.5, 0, 0.5, 1)$?\n",
    "- how big does the sample have to be to make the correlation apparent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marginals = np.array([1,1])\n",
    "rho = 0.9\n",
    "cov = np.array([[marginals[0]**2, rho*marginals[0]*marginals[1]], [rho*marginals[0]*marginals[1], marginals[1]**2]])\n",
    "dist = stats.multivariate_normal(mean=mu, cov=cov)\n",
    "draws = dist.rvs(size=10000, random_state=3)\n",
    "\n",
    "df = pd.DataFrame(draws, columns=['x', 'y'])\n",
    "sns.pairplot(df, kind='kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do different multivariate distributions look? Visualize a few of these, like the uniform distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = np.random.rand(2, 1000)\n",
    "df = pd.DataFrame({'x': x, 'y': y})\n",
    "sns.pairplot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Hard Question\n",
    "\n",
    "If a distribution doesn't have a finite mean and variance, like the Cauchy distriution, the central limit theorem doesn't hold. This is alarmingly common in the real world: so-called \"heavy-tailed\" distributions are everywhere, from the sizes of earthquakes to the sizes of cities, and you must be aware of them!\n",
    "\n",
    "What does this look like? You might like to explore what the draws from a Cauchy (or, google it: other heavy-tailed distributions) look like, and how the central limit theorem fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
